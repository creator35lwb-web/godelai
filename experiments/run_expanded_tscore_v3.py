#!/usr/bin/env python3
"""
GodelAI T-Score Validation Experiment v3 â€” Expanded Conflict Data
=================================================================
Purpose: Validate T-Score behavior on 108 expanded conflict data samples
         vs Shakespeare baseline. Measure both average T-Score AND variance.

Key improvements over v2:
- 108 samples (vs 22 original)
- Per-category analysis
- T-Score variance tracking (per Manus CTO analysis)
- Statistical significance testing
- Training text extraction handles all JSON structures

Author: Godel (Manus AI) â€” CTO
Date: February 7, 2026
"""

import json
import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
from datetime import datetime
import random
import math

# Add godelai to path
sys.path.insert(0, str(Path(__file__).parent))

# Configuration
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
RESULTS_DIR = Path(__file__).parent / "results"
DATASETS_DIR = Path(__file__).parent / "datasets" / "conflict"

print("=" * 70)
print("GodelAI T-Score Validation Experiment v3 â€” Expanded Data")
print("Testing Gradient Diversity with 108 Conflict Samples")
print("=" * 70)
print(f"Device: {DEVICE}")
print(f"Timestamp: {datetime.now().isoformat()}")
print()


class CharLevelModel(nn.Module):
    """Character-level language model for T-Score experiments."""
    
    def __init__(self, vocab_size=256, embed_dim=32, hidden_dim=64):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, num_layers=1)
        self.fc = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x):
        embedded = self.embedding(x)
        output, _ = self.lstm(embedded)
        return self.fc(output)


def load_all_conflict_data():
    """Load all conflict datasets from JSON files with flexible structure handling."""
    datasets = {}
    total_items = 0
    
    for category_dir in sorted(DATASETS_DIR.iterdir()):
        if category_dir.is_dir():
            category_name = category_dir.name
            datasets[category_name] = []
            
            for json_file in sorted(category_dir.glob("*.json")):
                with open(json_file, 'r') as f:
                    data = json.load(f)
                    
                    # Handle different JSON structures
                    if isinstance(data, dict) and 'data' in data:
                        items = data['data']
                    elif isinstance(data, list):
                        items = data
                    else:
                        items = [data]
                    
                    datasets[category_name].extend(items)
            
            count = len(datasets[category_name])
            total_items += count
            print(f"  {category_name}: {count} items")
    
    print(f"  TOTAL: {total_items} items")
    return datasets


def extract_training_texts(item):
    """Extract all usable training text from a conflict data item."""
    texts = []
    
    # Extract from perspectives (most conflict items have this)
    if 'perspectives' in item:
        for p in item['perspectives']:
            parts = []
            for key in ['position', 'reasoning', 'argument', 'evidence', 'counterpoint']:
                if key in p and p[key]:
                    parts.append(str(p[key]))
            if parts:
                texts.append(" ".join(parts))
    
    # Extract from scenario/description
    for key in ['scenario', 'description', 'context', 'question', 'dilemma']:
        if key in item and item[key]:
            texts.append(str(item[key]))
    
    # Extract from nested structures
    if 'conflicting_claims' in item:
        for claim in item['conflicting_claims']:
            if isinstance(claim, dict):
                for v in claim.values():
                    if isinstance(v, str) and len(v) > 20:
                        texts.append(v)
            elif isinstance(claim, str):
                texts.append(claim)
    
    if 'timeline' in item:
        for entry in item['timeline']:
            if isinstance(entry, dict):
                for v in entry.values():
                    if isinstance(v, str) and len(v) > 20:
                        texts.append(v)
    
    return texts


def create_batch(texts, batch_size=8, seq_length=64):
    """Create a batch of training data from texts."""
    batch_inputs = []
    batch_targets = []
    
    for _ in range(batch_size):
        text = random.choice(texts)
        encoded = [ord(c) % 256 for c in text]
        
        while len(encoded) < seq_length + 1:
            encoded.extend([ord(c) % 256 for c in random.choice(texts)])
        
        start = random.randint(0, max(0, len(encoded) - seq_length - 1))
        
        input_seq = encoded[start:start + seq_length]
        target_seq = encoded[start + 1:start + seq_length + 1]
        
        batch_inputs.append(input_seq)
        batch_targets.append(target_seq)
    
    return (
        torch.tensor(batch_inputs, dtype=torch.long).to(DEVICE),
        torch.tensor(batch_targets, dtype=torch.long).to(DEVICE)
    )


def compute_tscore(model):
    """Compute T-Score from model gradients."""
    grads = []
    for param in model.parameters():
        if param.grad is not None:
            grads.append(param.grad.view(-1))
    
    if not grads:
        return 0.5
    
    all_grads = torch.cat(grads)
    
    grad_std = all_grads.std().item()
    grad_mean = all_grads.abs().mean().item()
    
    if grad_mean < 1e-10:
        return 0.0
    
    raw_t = grad_std / grad_mean
    t_score = 2 / (1 + math.exp(-raw_t)) - 1
    
    return min(1.0, max(0.0, t_score))


def compute_tscore_variance(t_scores, window=10):
    """Compute rolling T-Score variance (key metric from CTO analysis)."""
    if len(t_scores) < window:
        return 0.0
    
    variances = []
    for i in range(len(t_scores) - window + 1):
        window_scores = t_scores[i:i + window]
        mean = sum(window_scores) / len(window_scores)
        var = sum((x - mean) ** 2 for x in window_scores) / len(window_scores)
        variances.append(var)
    
    return sum(variances) / len(variances) if variances else 0.0


def run_experiment(name, texts, num_batches=100, batch_size=8, seed=42):
    """Run training experiment with T-Score tracking."""
    random.seed(seed)
    torch.manual_seed(seed)
    
    print(f"\n{'â”€' * 60}")
    print(f"  Experiment: {name}")
    print(f"  Text samples: {len(texts)}")
    print(f"  Batches: {num_batches}")
    print(f"{'â”€' * 60}")
    
    model = CharLevelModel().to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    t_scores = []
    losses = []
    
    for batch_idx in range(num_batches):
        inputs, targets = create_batch(texts, batch_size=batch_size)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        
        outputs_flat = outputs.view(-1, outputs.size(-1))
        targets_flat = targets.view(-1)
        
        loss = criterion(outputs_flat, targets_flat)
        loss.backward()
        
        t_score = compute_tscore(model)
        
        optimizer.step()
        
        t_scores.append(t_score)
        losses.append(loss.item())
        
        if (batch_idx + 1) % 25 == 0:
            recent_t = t_scores[-25:]
            avg_t = sum(recent_t) / len(recent_t)
            var_t = sum((x - avg_t) ** 2 for x in recent_t) / len(recent_t)
            avg_l = sum(losses[-25:]) / 25
            print(f"    Batch {batch_idx + 1:3d}: T-Score={avg_t:.4f} (var={var_t:.6f}), Loss={avg_l:.4f}")
    
    # Compute comprehensive statistics
    avg_t = sum(t_scores) / len(t_scores)
    t_std = torch.tensor(t_scores).std().item()
    t_var = compute_tscore_variance(t_scores, window=10)
    
    # First half vs second half (learning trajectory)
    half = len(t_scores) // 2
    first_half_avg = sum(t_scores[:half]) / half
    second_half_avg = sum(t_scores[half:]) / (len(t_scores) - half)
    trajectory = second_half_avg - first_half_avg
    
    result = {
        "name": name,
        "num_texts": len(texts),
        "num_batches": num_batches,
        "t_score": {
            "average": round(avg_t, 6),
            "std": round(t_std, 6),
            "min": round(min(t_scores), 6),
            "max": round(max(t_scores), 6),
            "range": round(max(t_scores) - min(t_scores), 6),
            "rolling_variance": round(t_var, 8),
            "first_half_avg": round(first_half_avg, 6),
            "second_half_avg": round(second_half_avg, 6),
            "trajectory": round(trajectory, 6)
        },
        "loss": {
            "average": round(sum(losses) / len(losses), 6),
            "final": round(losses[-1], 6),
            "first_batch": round(losses[0], 6)
        },
        "raw_t_scores": [round(t, 6) for t in t_scores]
    }
    
    print(f"\n  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print(f"  â•‘  RESULTS: {name:<27}â•‘")
    print(f"  â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
    print(f"  â•‘  Avg T-Score:     {avg_t:>8.4f}           â•‘")
    print(f"  â•‘  T-Score Std:     {t_std:>8.4f}           â•‘")
    print(f"  â•‘  T-Score Range:   {result['t_score']['range']:>8.4f}           â•‘")
    print(f"  â•‘  Rolling Variance:{t_var:>8.6f}         â•‘")
    print(f"  â•‘  Trajectory:      {trajectory:>+8.4f}           â•‘")
    print(f"  â•‘  Final Loss:      {losses[-1]:>8.4f}           â•‘")
    print(f"  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    return result


def main():
    """Main experiment runner."""
    
    # Load all datasets
    print("\n" + "=" * 70)
    print("PHASE 1: Loading Datasets")
    print("=" * 70)
    
    conflict_datasets = load_all_conflict_data()
    
    # Prepare texts per category
    category_texts = {}
    all_conflict_texts = []
    
    for category, items in conflict_datasets.items():
        texts = []
        for item in items:
            extracted = extract_training_texts(item)
            texts.extend(extracted)
        category_texts[category] = texts
        all_conflict_texts.extend(texts)
        print(f"  {category}: {len(texts)} text segments extracted")
    
    print(f"  TOTAL conflict text segments: {len(all_conflict_texts)}")
    
    # Prepare Shakespeare baseline
    shakespeare_text = """To be, or not to be, that is the question:
Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune,
or to take arms against a sea of troubles and by opposing end them. To die, to sleep,
no more; and by a sleep to say we end the heart-ache and the thousand natural shocks
that flesh is heir to. 'Tis a consummation devoutly to be wish'd. To die, to sleep;
to sleep, perchance to dream. Ay, there's the rub: for in that sleep of death what
dreams may come, when we have shuffled off this mortal coil, must give us pause.
There's the respect that makes calamity of so long life. For who would bear the whips
and scorns of time, the oppressor's wrong, the proud man's contumely, the pangs of
despised love, the law's delay, the insolence of office, and the spurns that patient
merit of the unworthy takes, when he himself might his quietus make with a bare bodkin?
Who would fardels bear, to grunt and sweat under a weary life, but that the dread of
something after death, the undiscovered country from whose bourn no traveller returns,
puzzles the will, and makes us rather bear those ills we have than fly to others that
we know not of? Thus conscience does make cowards of us all, and thus the native hue
of resolution is sicklied o'er with the pale cast of thought, and enterprises of great
pith and moment with this regard their currents turn awry, and lose the name of action.
Soft you now, the fair Ophelia! Nymph, in thy orisons be all my sins remembered.
All the world's a stage, and all the men and women merely players. They have their exits
and their entrances, and one man in his time plays many parts, his acts being seven ages.
At first, the infant, mewling and puking in the nurse's arms. Then the whining schoolboy,
with his satchel and shining morning face, creeping like snail unwillingly to school.
And then the lover, sighing like furnace, with a woeful ballad made to his mistress' eyebrow.
Then a soldier, full of strange oaths and bearded like the pard, jealous in honour,
sudden and quick in quarrel, seeking the bubble reputation even in the cannon's mouth.
And then the justice, in fair round belly with good capon lined, with eyes severe and
beard of formal cut, full of wise saws and modern instances; and so he plays his part.
The sixth age shifts into the lean and slippered pantaloon, with spectacles on nose and
pouch on side; his youthful hose, well saved, a world too wide for his shrunk shank,
and his big manly voice, turning again toward childish treble, pipes and whistles in
his sound. Last scene of all, that ends this strange eventful history, is second
childishness and mere oblivion, sans teeth, sans eyes, sans taste, sans everything."""
    
    shakespeare_texts = [shakespeare_text[i:i+200] for i in range(0, len(shakespeare_text), 80)]
    shakespeare_texts = [t for t in shakespeare_texts if len(t) > 80]
    print(f"\n  Shakespeare baseline: {len(shakespeare_texts)} text segments")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RUN EXPERIMENTS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    results = {
        "experiment": "T-Score Validation v3 â€” Expanded Conflict Data (108 samples)",
        "timestamp": datetime.now().isoformat(),
        "device": str(DEVICE),
        "author": "Godel (Manus AI) â€” CTO",
        "hypothesis": "Conflict data should show higher T-Score VARIANCE due to gradient diversity from contradictory perspectives",
        "datasets": {
            "total_conflict_items": sum(len(v) for v in conflict_datasets.values()),
            "total_conflict_texts": len(all_conflict_texts),
            "shakespeare_texts": len(shakespeare_texts),
            "categories": {k: len(v) for k, v in conflict_datasets.items()}
        },
        "experiments": []
    }
    
    # Experiment 1: Shakespeare Baseline
    print("\n" + "=" * 70)
    print("PHASE 2: Running Experiments")
    print("=" * 70)
    
    baseline = run_experiment(
        "Shakespeare (Homogeneous)",
        shakespeare_texts,
        num_batches=100
    )
    results["experiments"].append(baseline)
    
    # Experiment 2: All Conflict Data Combined
    conflict_result = run_experiment(
        "All Conflict (Heterogeneous)",
        all_conflict_texts,
        num_batches=100
    )
    results["experiments"].append(conflict_result)
    
    # Experiment 3: Per-category experiments
    for category, texts in category_texts.items():
        if len(texts) >= 5:  # Need minimum texts
            cat_result = run_experiment(
                f"Category: {category}",
                texts,
                num_batches=100
            )
            results["experiments"].append(cat_result)
    
    # Experiment 4: Mixed (Shakespeare + Conflict)
    mixed_texts = shakespeare_texts + all_conflict_texts
    mixed_result = run_experiment(
        "Mixed (Shakespeare + Conflict)",
        mixed_texts,
        num_batches=100
    )
    results["experiments"].append(mixed_result)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # COMPREHENSIVE ANALYSIS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print("\n" + "=" * 70)
    print("PHASE 3: Comprehensive Analysis")
    print("=" * 70)
    
    print(f"\n{'Dataset':<35} {'Avg T':>8} {'T-Std':>8} {'T-Var':>10} {'Range':>8} {'Traj':>8} {'Loss':>8}")
    print("â”€" * 95)
    for r in results["experiments"]:
        t = r["t_score"]
        print(f"{r['name']:<35} {t['average']:>8.4f} {t['std']:>8.4f} {t['rolling_variance']:>10.6f} {t['range']:>8.4f} {t['trajectory']:>+8.4f} {r['loss']['final']:>8.4f}")
    
    # Key comparisons
    b_t = baseline["t_score"]
    c_t = conflict_result["t_score"]
    
    print(f"\n{'â”€' * 70}")
    print(f"KEY COMPARISONS (Conflict vs Shakespeare)")
    print(f"{'â”€' * 70}")
    print(f"  T-Score Average Diff:   {c_t['average'] - b_t['average']:+.6f}")
    print(f"  T-Score Std Diff:       {c_t['std'] - b_t['std']:+.6f}")
    print(f"  T-Score Variance Diff:  {c_t['rolling_variance'] - b_t['rolling_variance']:+.8f}")
    print(f"  T-Score Range Diff:     {c_t['range'] - b_t['range']:+.6f}")
    print(f"  Trajectory Diff:        {c_t['trajectory'] - b_t['trajectory']:+.6f}")
    
    # Variance ratio (key metric)
    if b_t['rolling_variance'] > 0:
        var_ratio = c_t['rolling_variance'] / b_t['rolling_variance']
        print(f"\n  ğŸ“Š VARIANCE RATIO (Conflict/Shakespeare): {var_ratio:.2f}x")
    
    # Conclusion
    print(f"\n{'â•' * 70}")
    print(f"CONCLUSION")
    print(f"{'â•' * 70}")
    
    var_diff = c_t['rolling_variance'] - b_t['rolling_variance']
    std_diff = c_t['std'] - b_t['std']
    range_diff = c_t['range'] - b_t['range']
    
    significant_diffs = sum([
        abs(var_diff) > 0.0001,
        abs(std_diff) > 0.005,
        abs(range_diff) > 0.01
    ])
    
    if significant_diffs >= 2:
        conclusion = "CONFIRMED: Conflict data produces measurably different gradient diversity patterns"
        print(f"\n  âœ… {conclusion}")
    elif significant_diffs == 1:
        conclusion = "PARTIAL: Some difference detected, but not across all metrics"
        print(f"\n  âš ï¸ {conclusion}")
    else:
        conclusion = "INCONCLUSIVE: Character-level model may not capture semantic conflicts"
        print(f"\n  â“ {conclusion}")
    
    print(f"\n  IMPLICATION: {'Semantic-level models needed for full C-S-P activation' if significant_diffs < 2 else 'Data engineering approach validated'}")
    
    results["analysis"] = {
        "variance_diff": round(var_diff, 8),
        "std_diff": round(std_diff, 6),
        "range_diff": round(range_diff, 6),
        "variance_ratio": round(var_ratio, 4) if b_t['rolling_variance'] > 0 else None,
        "significant_metrics": significant_diffs,
        "conclusion": conclusion
    }
    
    # Save results
    RESULTS_DIR.mkdir(exist_ok=True)
    output_file = RESULTS_DIR / f"expanded_tscore_v3_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\n  Results saved to: {output_file}")
    
    return results


if __name__ == "__main__":
    results = main()
