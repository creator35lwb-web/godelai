{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIWllCUCnYBaOkhwzG9u1S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/creator35lwb-web/godelai/blob/main/GODELAI_Mirror_Test_01_by_Alton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OiFnt7Lsagn",
        "outputId": "5627350d-570b-4e09-ce40-f0fb125fb3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'godelai'...\n",
            "remote: Enumerating objects: 142, done.\u001b[K\n",
            "remote: Counting objects: 100% (142/142), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 142 (delta 51), reused 111 (delta 20), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (142/142), 7.79 MiB | 23.81 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "/content/godelai/godelai/godelai\n",
            "âœ… GodelAI Environment Ready.\n",
            "   Repository cloned from: https://github.com/creator35lwb-web/godelai\n",
            "============================================================\n",
            "ðŸš€ LIVE EXECUTION: GodelAI Mirror Test\n",
            "============================================================\n",
            "\n",
            ">>> [SYSTEM ALERT] Wisdom Critical (T < 0.80). Triggering Sleep Protocol...\n",
            ">>> [Godel] Sleeping... Pruning noise and restoring surplus energy.\n",
            ">>> [Godel] Woke up. Clarity restored.\n",
            "\n",
            "Step 01 | Wisdom: 0.7311 | ðŸ’¤ REFLECTING\n",
            "\n",
            ">>> [SYSTEM ALERT] Wisdom Critical (T < 0.80). Triggering Sleep Protocol...\n",
            ">>> [Godel] Sleeping... Pruning noise and restoring surplus energy.\n",
            ">>> [Godel] Woke up. Clarity restored.\n",
            "\n",
            "Step 02 | Wisdom: 0.7311 | ðŸ’¤ REFLECTING\n",
            "\n",
            ">>> [SYSTEM ALERT] Wisdom Critical (T < 0.80). Triggering Sleep Protocol...\n",
            ">>> [Godel] Sleeping... Pruning noise and restoring surplus energy.\n",
            ">>> [Godel] Woke up. Clarity restored.\n",
            "\n",
            "Step 03 | Wisdom: 0.7311 | ðŸ’¤ REFLECTING\n",
            "\n",
            ">>> [SYSTEM ALERT] Wisdom Critical (T < 0.80). Triggering Sleep Protocol...\n",
            ">>> [Godel] Sleeping... Pruning noise and restoring surplus energy.\n",
            ">>> [Godel] Woke up. Clarity restored.\n",
            "\n",
            "Step 04 | Wisdom: 0.7311 | ðŸ’¤ REFLECTING\n",
            "\n",
            "âœ… Execution Verified on Public Colab Runtime.\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install & Setup GodelAI Environment\n",
        "# This cell clones the repo directly from your new public GitHub\n",
        "!git clone https://github.com/creator35lwb-web/godelai.git\n",
        "%cd godelai\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the path so Python can find the godelai module\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "print(\"âœ… GodelAI Environment Ready.\")\n",
        "print(\"   Repository cloned from: https://github.com/creator35lwb-web/godelai\")\n",
        "\n",
        "# @title 2. Run the 'Mirror Test' (The AI Reads Its Soul)\n",
        "# This executes the exact logic from your test_mirror.py\n",
        "from godelai.agent import GodelAgent\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Re-defining the simple components here to ensure standalone execution in Colab\n",
        "class SimpleTextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size=128, embed_dim=32, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        encoded, _ = self.encoder(embedded)\n",
        "        return self.output(encoded[:, -1, :])\n",
        "\n",
        "def text_to_tensor(text, max_len=100):\n",
        "    chars = [ord(c) % 128 for c in text[:max_len]]\n",
        "    return torch.tensor(chars).unsqueeze(0)\n",
        "\n",
        "# The Whitepaper Text\n",
        "WHITEPAPER_EXCERPT = \"\"\"\n",
        "GodelAI: The Architecture of Inheritance. Wisdom is not an existence. It is a process structure...\n",
        "The C-S-P Framework: Compression, State, Propagation...\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸš€ LIVE EXECUTION: GodelAI Mirror Test\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "base_model = SimpleTextEncoder()\n",
        "# Use a sensitive threshold to demonstrate dynamics\n",
        "agent = GodelAgent(base_model, min_surplus_energy=0.1)\n",
        "agent.epsilon = 0.8 # Set high to force some reflection events for demo\n",
        "agent.optimizer = optim.Adam(agent.compression_layer.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "sentences = [s.strip() for s in WHITEPAPER_EXCERPT.split('.') if len(s.strip()) > 10]\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    input_tensor = text_to_tensor(sentence)\n",
        "    target = torch.randint(0, 128, (1,))\n",
        "\n",
        "    loss, wisdom_score, status = agent.learning_step(input_tensor, target, criterion)\n",
        "\n",
        "    # Visualizing the Pulse\n",
        "    bar = \"â–ˆ\" * int(wisdom_score * 20)\n",
        "    icon = \"ðŸ’¤ REFLECTING\" if status == \"SLEEP\" else \"âš¡ ENGAGING\"\n",
        "\n",
        "    print(f\"Step {i+1:02d} | Wisdom: {wisdom_score:.4f} | {icon}\")\n",
        "\n",
        "print(\"\\nâœ… Execution Verified on Public Colab Runtime.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "409c3d86",
        "outputId": "6378c7ce-6160-4aa9-fb4a-e65f3b5e855c"
      },
      "source": [
        "import inspect\n",
        "from godelai.agent import GodelAgent\n",
        "\n",
        "# Get all members of the GodelAgent class\n",
        "members = inspect.getmembers(GodelAgent, predicate=inspect.isfunction)\n",
        "\n",
        "print(\"Public methods of GodelAgent:\")\n",
        "for name, member in members:\n",
        "    if not name.startswith('_'): # Exclude private/protected methods\n",
        "        print(f\"- {name}: {inspect.getdoc(member).splitlines()[0] if inspect.getdoc(member) else 'No description available.'}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public methods of GodelAgent:\n",
            "- add_module: Add a child module to the current module.\n",
            "- apply: Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
            "- bfloat16: Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
            "- buffers: Return an iterator over module buffers.\n",
            "- children: Return an iterator over immediate children modules.\n",
            "- compile: Compile this Module's forward using :func:`torch.compile`.\n",
            "- cpu: Move all model parameters and buffers to the CPU.\n",
            "- cuda: Move all model parameters and buffers to the GPU.\n",
            "- double: Casts all floating point parameters and buffers to ``double`` datatype.\n",
            "- eval: Set the module in evaluation mode.\n",
            "- extra_repr: Return the extra representation of the module.\n",
            "- float: Casts all floating point parameters and buffers to ``float`` datatype.\n",
            "- forward: Define the computation performed at every call.\n",
            "- get_buffer: Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
            "- get_extra_state: Return any extra state to include in the module's state_dict.\n",
            "- get_parameter: Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
            "- get_submodule: Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
            "- get_training_summary: Get a summary of the training history.\n",
            "- half: Casts all floating point parameters and buffers to ``half`` datatype.\n",
            "- ipu: Move all model parameters and buffers to the IPU.\n",
            "- learning_step: The Evolution Step (Forward + Backward + Wisdom Check).\n",
            "- load_state_dict: Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
            "- measure_gradient_diversity: Implementation of the Wisdom Metric (Option B: Gradient Diversity).\n",
            "- modules: Return an iterator over all modules in the network.\n",
            "- mtia: Move all model parameters and buffers to the MTIA.\n",
            "- named_buffers: Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
            "- named_children: Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
            "- named_modules: Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
            "- named_parameters: Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
            "- parameters: Return an iterator over module parameters.\n",
            "- register_backward_hook: Register a backward hook on the module.\n",
            "- register_buffer: Add a buffer to the module.\n",
            "- register_forward_hook: Register a forward hook on the module.\n",
            "- register_forward_pre_hook: Register a forward pre-hook on the module.\n",
            "- register_full_backward_hook: Register a backward hook on the module.\n",
            "- register_full_backward_pre_hook: Register a backward pre-hook on the module.\n",
            "- register_load_state_dict_post_hook: Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n",
            "- register_load_state_dict_pre_hook: Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n",
            "- register_module: Alias for :func:`add_module`.\n",
            "- register_parameter: Add a parameter to the module.\n",
            "- register_state_dict_post_hook: Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
            "- register_state_dict_pre_hook: Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
            "- requires_grad_: Change if autograd should record operations on parameters in this module.\n",
            "- rest_and_reflect: The Sleep Protocol (Option 1: Pruning-based).\n",
            "- set_extra_state: Set extra state contained in the loaded `state_dict`.\n",
            "- set_submodule: Set the submodule given by ``target`` if it exists, otherwise throw an error.\n",
            "- share_memory: See :meth:`torch.Tensor.share_memory_`.\n",
            "- state_dict: Return a dictionary containing references to the whole state of the module.\n",
            "- to: Move and/or cast the parameters and buffers.\n",
            "- to_empty: Move the parameters and buffers to the specified device without copying storage.\n",
            "- train: Set the module in training mode.\n",
            "- type: Casts all parameters and buffers to :attr:`dst_type`.\n",
            "- xpu: Move all model parameters and buffers to the XPU.\n",
            "- zero_grad: Reset gradients of all model parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45a24b8b",
        "outputId": "d2bcdf69-764c-4381-916d-8e93fb33cc65"
      },
      "source": [
        "from godelai.agent import GodelAgent\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "# Re-using the SimpleTextEncoder and text_to_tensor\n",
        "class SimpleTextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size=128, embed_dim=32, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        encoded, _ = self.encoder(embedded)\n",
        "        return self.output(encoded[:, -1, :])\n",
        "\n",
        "def text_to_tensor(text, max_len=100):\n",
        "    chars = [ord(c) % 128 for c in text[:max_len]]\n",
        "    return torch.tensor(chars).unsqueeze(0)\n",
        "\n",
        "# Initialize the agent and its components for this experiment\n",
        "base_model_mse_exp = SimpleTextEncoder()\n",
        "agent_mse_exp = GodelAgent(base_model_mse_exp, min_surplus_energy=0.1) # Start with default\n",
        "agent_mse_exp.optimizer = optim.Adam(agent_mse_exp.compression_layer.parameters(), lr=0.001)\n",
        "criterion_mse_exp = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set epsilon to a value that would normally cause reflection if surplus energy is low\n",
        "agent_mse_exp.epsilon = 0.8 # High epsilon to make reflection more likely\n",
        "\n",
        "sample_text_mse = \"Exploring the impact of min_surplus_energy.\"\n",
        "input_tensor_mse = text_to_tensor(sample_text_mse)\n",
        "target_mse = torch.randint(0, 128, (1,))\n",
        "\n",
        "print(\"\\n--- Experiment 1: High min_surplus_energy ---\")\n",
        "agent_mse_exp.min_surplus_energy = 0.5 # Set a higher threshold\n",
        "print(f\"Set agent.min_surplus_energy to: {agent_mse_exp.min_surplus_energy}\")\n",
        "loss1, wisdom_score1, status1 = agent_mse_exp.learning_step(input_tensor_mse, target_mse, criterion_mse_exp)\n",
        "print(f\"Loss: {loss1:.4f}, Wisdom: {wisdom_score1:.4f}, Status: {status1}\")\n",
        "\n",
        "print(\"\\n--- Experiment 2: Low min_surplus_energy ---\")\n",
        "agent_mse_exp.min_surplus_energy = 0.01 # Set a lower threshold\n",
        "print(f\"Set agent.min_surplus_energy to: {agent_mse_exp.min_surplus_energy}\")\n",
        "loss2, wisdom_score2, status2 = agent_mse_exp.learning_step(input_tensor_mse, target_mse, criterion_mse_exp)\n",
        "print(f\"Loss: {loss2:.4f}, Wisdom: {wisdom_score2:.4f}, Status: {status2}\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Experiment 1: High min_surplus_energy ---\n",
            "Set agent.min_surplus_energy to: 0.5\n",
            "\n",
            ">>> [SYSTEM ALERT] Wisdom Critical (T < 0.80). Triggering Sleep Protocol...\n",
            ">>> [Godel] Sleeping... Pruning noise and restoring surplus energy.\n",
            ">>> [Godel] Woke up. Clarity restored.\n",
            "\n",
            "Loss: 4.9197, Wisdom: 0.7311, Status: SLEEP\n",
            "\n",
            "--- Experiment 2: Low min_surplus_energy ---\n",
            "Set agent.min_surplus_energy to: 0.01\n",
            "\n",
            ">>> [SYSTEM ALERT] Wisdom Critical (T < 0.80). Triggering Sleep Protocol...\n",
            ">>> [Godel] Sleeping... Pruning noise and restoring surplus energy.\n",
            ">>> [Godel] Woke up. Clarity restored.\n",
            "\n",
            "Loss: 4.9197, Wisdom: 0.7311, Status: SLEEP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e16be42",
        "outputId": "9d6d7ee3-10a6-40ca-8aad-a24290916e1e"
      },
      "source": [
        "from godelai.agent import GodelAgent\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "# Re-using the SimpleTextEncoder and text_to_tensor from the previous cell\n",
        "class SimpleTextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size=128, embed_dim=32, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        encoded, _ = self.encoder(embedded)\n",
        "        return self.output(encoded[:, -1, :])\n",
        "\n",
        "def text_to_tensor(text, max_len=100):\n",
        "    chars = [ord(c) % 128 for c in text[:max_len]]\n",
        "    return torch.tensor(chars).unsqueeze(0)\n",
        "\n",
        "# Initialize the agent and its components\n",
        "base_model = SimpleTextEncoder()\n",
        "agent = GodelAgent(base_model, min_surplus_energy=0.1)\n",
        "agent.optimizer = optim.Adam(agent.compression_layer.parameters(), lr=0.001) # Optimizer for the compression layer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Experimenting with agent.epsilon ---\n",
        "print(f\"\\n--- Experimenting with agent.epsilon ---\")\n",
        "\n",
        "# Set epsilon to a lower value to see if it reduces reflection\n",
        "agent.epsilon = 0.05 # Previously it was implicitly higher in the 'Mirror Test'\n",
        "print(f\"Set agent.epsilon to: {agent.epsilon}\")\n",
        "\n",
        "# Prepare sample input\n",
        "sample_text = \"This is another sample sentence to test epsilon.\"\n",
        "input_tensor = text_to_tensor(sample_text)\n",
        "target = torch.randint(0, 128, (1,))\n",
        "\n",
        "# Call the learning_step method with the new epsilon\n",
        "loss, wisdom_score, status = agent.learning_step(input_tensor, target, criterion)\n",
        "\n",
        "print(f\"Loss: {loss:.4f}\")\n",
        "print(f\"Wisdom Score: {wisdom_score:.4f}\")\n",
        "print(f\"Agent Status: {status}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Experimenting with agent.epsilon ---\n",
            "Set agent.epsilon to: 0.05\n",
            "Loss: 5.4362\n",
            "Wisdom Score: 0.7311\n",
            "Agent Status: LEARN\n"
          ]
        }
      ]
    }
  ]
}